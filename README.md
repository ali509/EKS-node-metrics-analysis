This project aims to create a Kubernetes cron job that pulls node metrics such as CPU, memory, and disk usage from Kubernetes nodes and stores them in individual files. The cron job is designed to run at regular intervals,
creating a new file with a timestamp each time it executes. To collect and expose metrics, we will utilize Node Exporter, a popular tool in the Kubernetes ecosystem. The solution will be implemented using Python scripting for the cron job, Docker for containerization, and Kubernetes YAML deployment. The deployment can be performed on a local Kubernetes setup like Minikube or Kind, or on a cloud platform's Kubernetes flavor.

Here I have used EKS (Elastic kubernetes service).This document provides a step-by-step guide on the design, deployment, and usage of the solution, along with the necessary code and configuration files

<img width="289" alt="image" src="https://github.com/ali509/EKS-node-metrics-analysis/assets/39634565/93518bb0-7d43-468b-a271-ae9ede4fb7f5">

## Components

1. **Dockerfile:**
   - `dockerfile`: Defines the steps to build a Docker image. I have used Dockerhub to push image can use ECR as well.
   - `python_script.py`: Python script responsible for collecting node metrics, filtering, and storing them in files.
   - `requirements.txt`: Contains dependencies required by the Python script.

2. **Manifest Files:**
   - `cron.yaml`: Defines a Kubernetes cron job scheduled to run at regular intervals. It executes the Python script to collect node metrics and store them in files.
   - `nodemetrics.yaml`: YAML file for pod creation to store logs files generated by Kubernetes cron.
   - `pvc.yaml`: YAML file defining a Persistent Volume Claim (PVC) used for storing metric data persistently.

3. **Node Exporter:**
   - `configmap.yaml`: Configures Node Exporter.
   - `daemonset.yaml`: Deploys Node Exporter as a DaemonSet on all nodes in the Kubernetes cluster.
   - `load_balancer.yaml`: Contains resources for load balancing or exposing Node Exporter metrics externally.

## Installed Components for the Project

1. **Metric Server:**
   - **Purpose:** Collects resource metrics for pods and nodes.
   - **Installation:** Enables monitoring and autoscaling in the cluster.

2. **EBS CSI Driver Addon:**
   - **Purpose:** Manages Amazon EBS volumes for persistent storage.
   - **Installation:** Facilitates dynamic provisioning of storage volumes.

3. **Bastion Docker:**
   - **Purpose:** Acts as a secure access point to the Kubernetes cluster.
   - **Installation:** Provides a controlled environment for administrative tasks.

4. **Python:**
   - **Purpose:** Used for scripting, automation, and data processing.
   - **Installation:** Supports custom scripting and integration with Kubernetes.

## Steps After Setup

1. **Build Docker Image:**
   - Use the provided Dockerfile to build a Docker image containing the necessary components, including the Python script and any dependencies.
   - Run the `docker build` command to build the Docker image locally on your development machine.

2. **Push Image to Docker Hub:**
   - After building the Docker image, tag it appropriately with your Docker Hub username/repository.
   - Use the `docker push` command to push the Docker image to Docker Hub, making it accessible for deployment in Kubernetes.

3. **Apply Manifest Files:**
   - Use the `kubectl apply` command to apply the manifest files located in the project directory.
   - Apply the PVC YAML file to claim storage for metric data persistence.
   - Apply the cron job YAML file to schedule the execution of the Python script for collecting node metrics.
   - Apply any additional manifest files, such as those for Node Exporter or other related resources.

4. **Sanity Checking:**
   - After applying the manifest files, perform a sanity check to ensure that all pods and resources are running as expected.
   - Use the `kubectl get pods` command to check the status of pods and ensure that they are all in the Running state.
   - Use the `kubectl describe pod` command to inspect the logs and debug any issues if pods are not running correctly.

5. **Debugging and Logging:**
   - If any pods are not running or encountering issues, inspect the logs of the relevant pods using the `kubectl logs` command.
   - Debug any errors or issues reported in the logs to identify and resolve potential issues with the deployment.

6. **Access Node Exporter URL:**
   - Once all pods are running successfully, access the URL of Node Exporter to verify that it is exposing metrics correctly.
   - Open a web browser and navigate to the URL of Node Exporter, typically in the format `http://<node-exporter-ip>:<port>/metrics`.
   - Verify that metrics are being exposed and can be accessed from the Node Exporter endpoint.

7. **Validation and Monitoring:**
   - Validate the functionality of the cron job by checking that metric data is being collected and stored persistently in the PVC.
   - Monitor the Kubernetes cluster and node metrics over time to ensure
